{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset = load_dataset('cats_vs_dogs')\n",
    "cat_dataset = hf_dataset.filter(lambda example: example['labels'] == 0)\n",
    "dog_dataset = hf_dataset.filter(lambda example: example['labels'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- The original dataset contains two features: `image` and `labels`\n",
    "    - `image`: Consists of cats and dogs from PetFinder.com\n",
    "        - Images in the dataset have varying heights and widths. There are 412 unique heights, 429 unique widths, and 3635 unique dimensions. This means that our images will have varying levels of detail.\n",
    "        - Min height & width: 4, 4. Max height & width: 500, 500.\n",
    "    - `labels`: 0 = cat, 1 = dog\n",
    "- Number of cats: 11,741; Number of dogs: 11,669\n",
    "    - This indicates a nearly-balanced dataset between cats and dogs. It is however unclear whether there is a balance between cat and dog breeds.\n",
    "- Initial clustering over the cat dataset did not contain separable clusters.\n",
    "    - The model used to construct image embeddings is Google's SigLIP (`siglip-base-patch16-256`).\n",
    "    - Number of clusters was varied between 3 to 50 clusters\n",
    "    - Dimensionality reduction was done using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = []\n",
    "widths = []\n",
    "dimensions = set()\n",
    "\n",
    "for inst in cat_dataset['train']:\n",
    "    heights.append(inst['image'].size[0])\n",
    "    widths.append(inst['image'].size[1])\n",
    "    dimensions.add(inst['image'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dataframe = cat_dataset['train'].to_pandas()\n",
    "cat_dataframe['height'] = heights\n",
    "cat_dataframe['width'] = widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dataframe.height.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dataframe.width.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_dataset.filter(lambda example: example['image'].size[0] > 100 and example['image'].size[1] > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipVisionModel\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "# DEVICE = torch.device(\n",
    "#     'cuda' if torch.cuda.is_available() \\\n",
    "#         else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "MODEL = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-256\").to(DEVICE)\n",
    "PROCESSOR = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL_TO_TENSOR = torchvision.transforms.PILToTensor()\n",
    "\n",
    "def transform(examples):\n",
    "    images = [PIL_TO_TENSOR(image.convert('RGB')) for image in examples['image']]\n",
    "    return {'image': images}\n",
    "\n",
    "cat_dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_embeddings(model: torch.nn.Module, images: Dataset):\n",
    "    \"\"\"\n",
    "    Get image embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    # dataloader = torch.utils.data.DataLoader(\n",
    "    #     images, batch_size=CONFIG.per_device_train_batch_size,\n",
    "    #     shuffle=False)\n",
    "\n",
    "    embeddings = None\n",
    "    for inst in tqdm(images, total=len(images)):\n",
    "        inputs = PROCESSOR(images=inst['image'], return_tensors=\"pt\")\n",
    "        output = model(**inputs)\n",
    "        embeddings = output.pooler_output if embeddings is None \\\n",
    "            else torch.cat([embeddings, output.pooler_output], dim=0)\n",
    "\n",
    "    print(f\"Embeddings: {embeddings.shape}\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "SEED = 1\n",
    "CONFIG = Namespace(\n",
    "    seed=SEED,\n",
    "    min_num_clusters = 3,\n",
    "    max_num_clusters = 50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "img_embeddings = get_embeddings(MODEL, cat_dataset['train'])\n",
    "normalized_img_embeddings = normalize(img_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "def run_kmedoids(embeddings, num_clusters=8):\n",
    "    \"\"\"\n",
    "    Train KMedoids\n",
    "    \"\"\"\n",
    "\n",
    "    kmedoids = KMedoids(n_clusters=num_clusters, metric='cosine')\n",
    "    kmedoids.fit(embeddings)\n",
    "    labels = kmedoids.predict(embeddings)\n",
    "\n",
    "    return kmedoids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for n_clusters in range(CONFIG.min_num_clusters, CONFIG.max_num_clusters+1):\n",
    "\n",
    "    _, labels = run_kmedoids(normalized_img_embeddings, num_clusters=n_clusters)\n",
    "    avg_score = silhouette_score(normalized_img_embeddings, labels, metric='cosine',\n",
    "                                 random_state=CONFIG.seed)\n",
    "    scores.append((n_clusters, avg_score))\n",
    "\n",
    "print(f\"Score: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sns.set(font_scale=2.5)\n",
    "plt.set_cmap('tab20')\n",
    "\n",
    "kmedoids_obj, labels = run_kmedoids(normalized_img_embeddings, num_clusters=5)\n",
    "\n",
    "color_map = plt.get_cmap('tab20')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "pca = TSNE(n_components=2)\n",
    "pca_embedding = pca.fit_transform(normalized_img_embeddings)\n",
    "df = pd.DataFrame(pca_embedding, columns=['pca1', 'pca2'])\n",
    "df['cluster'] = labels\n",
    "# df['sentence'] = [sent for sent, _ in sent_kb]\n",
    "\n",
    "plot_labels = df['cluster'].values\n",
    "num_labels = len(set(plot_labels))\n",
    "for i, label in enumerate(set(plot_labels)):\n",
    "    label_df = df[df.cluster == label]\n",
    "    ax.scatter(label_df.pca1, label_df.pca2, label=str(label), color=color_map(i))\n",
    "# ax.scatter(df.pca1, df.pca2)\n",
    "\n",
    "ax.set_xlabel(\"PCA Dimension 1\")\n",
    "ax.set_ylabel(\"PCA Dimension 2\")\n",
    "# ax.set_zlabel(\"PCA Dimension 3\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.7, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cat-generator-lINQe6Oj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
